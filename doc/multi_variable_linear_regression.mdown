<script type="text/x-mathjax-config">
   MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" 
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

# Multi-Variable Linear Regression


- Hypothesis using matrix
$$H(x_1,x_2,x_3) = x_1w_1+x_2w_2+x_3w_3$$  


    x1_data = [73,93,89,96,73]
    x2_data = [80,88,91,98,66]
    x3_data = [75,93,90,100,70]
    y_data = [152,185,180,196,142]

    # placeholders for a tensor that will be always fed.
    x1 = tf.placeholder(tf.float32)
    x2 = tf.placeholder(tf.float32)
    x2 = tf.placeholder(tf.float32)

    y = tf.placeholder(tf.float32)

    w1 = tf.Variable(tf.random_normal([1]),name='weight1')
    w2 = tf.Variable(tf.random_normal([1]),name='weight2')
    w3 = tf.Variable(tf.random_normal([1]),name='weight3')
    b = tf.Variable(tf.random_normal([1]),name='bias')

    hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b

    # cost/loss function
    cost = tf.reduce_mean(tf.square(hypothesis - Y))

    # Minimize. Need a very small learning rate for this data set
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)
    train = optimizer.minimize(cost)

    # Launch the graph in a session.
    sess = tf.Session()
    # Initializes global variables in the graph.
    sess.run(tf.global_variables_initializer())

    # 학습
    for step in range(2001):
        # cost,hypothesis 기록
        cost_val, hy_val, _ = sess.run([cost, hypothesis, train],
                            feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})
        if step % 10 == 0:
            print(step, "Cost: ", cost_val, "\nPrediction:\n", hy_val)


- Metrics
    + 위의 방법은 좋은 방법은 아니므로 행렬로 변환
$$(x_1x_2x_3)*\begin{pmatrix} w_1 \\\ w_2 \\\ w_3\end{pmatrix}=(x_1w_1+x_2w_2+x_3w_3)$$ $$H(X)=XW$$


    tf.set_random_seed(777)  # for reproducibility

    x_data = [[73., 80., 75.],[93., 88., 93.],[89., 91., 90.],[96., 98., 100.],[73., 66., 70.]]
    y_data = [[152.],[185.],[180.],[196.],[142.]]


    # placeholders for a tensor that will be always fed.
    # X(x_data)의 경우 element는 갯수의 제한이 없으므로 무한대(None), 열에 대한것은 3
    X = tf.placeholder(tf.float32, shape=[None, 3])
    Y = tf.placeholder(tf.float32, shape=[None, 1])

    W = tf.Variable(tf.random_normal([3, 1]), name='weight')
    b = tf.Variable(tf.random_normal([1]), name='bias')

    # Hypothesis
    hypothesis = tf.matmul(X, W) + b

    # Simplified cost/loss function
    cost = tf.reduce_mean(tf.square(hypothesis - Y))

    # Minimize
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)
    train = optimizer.minimize(cost)

    # Launch the graph in a session.
    sess = tf.Session()
    # Initializes global variables in the graph.
    sess.run(tf.global_variables_initializer())

    for step in range(2001):
        cost_val, hy_val, _ = sess.run(
            [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})
        if step % 10 == 0:
            print(step, "Cost: ", cost_val, "\nPrediction:\n", hy_val)
